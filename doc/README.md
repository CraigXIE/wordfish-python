## Overview of Project
I first tried setting up an application called DeepDive in a standard way, to perform extractions of mentions and features by way of the deepdive executable, to the database. The project was ultimately successful in that I completed all steps through inference, but it was too hard. I then had two choices - to be a user and try to refine my particular classifier, or try to improve the process (now that I understand it). I did not feel that I had the amount of control that would be desired, as the current infrastructure does not extend well to a SLURM environment with a launch setup. I realized very quickly that I was using the DeepDive command simply as a wrapper to write to the data base, and given that I had to write my own scripts anyway, it would be easiest to break apart the initial steps for extractions, build more "container-ized" methods to do initial steps, and then use DeepDive for the training and inference (where I see it's strength). It was also apparent that with the current setup, users would be doing different versions of the same thing, over again, and this is not efficient. I want to start from scratch with a modified infrastructure, and this gives me an opportunity to think about how I want to do this, because it needs to be a lot easier than it currently is. My first goal was to standardize the process, and build a set of python tools that can work with simple inputs and outputs for (what will eventually be) a cloud-based or VM-based deployment. Along the way, I realized that the model of first needing to run the Stanford parser, extract mentions and relationships, was not ideal. I want to flip it on its head, and first extract unsupervised features, for some user specified input corpus, and then map terms of interest into these features to assess relationships. I want to build a tool (likely with Docker) that will let others use this method on their data. 

My first projects will be done in the context of Neuroimaging / Psychology analysis, and that all steps should come from data structures (and not manually doing things).

### Infrastructure
The big picture idea is that a user will be walked through the process of setting up a deepdive deployment. He she will install this python module, open up a web interface, and choose one or more terminologies to determine "mentions," choose one or more sources of input data (corpus), and then the ultimate deployment environment. After this process, a custom instantiation of the module will be generated to install in the environment of choice. In the case that the user selects a virtual machine, the setup of databases / envionment will be done automatically on this VM (either local vagrant or cloud-aws). In the case of deployment on a cluster, a zipped up folder with the instantiation will be produced to be dropped onto the cluster, installed, and run. More details will follow.

#### Corpus and Terminology Plugins
I should be able to retrieve (programatically) from a selection of corpus and terminology. There should be a "plugins" folder with different wrappers for various corpus and terminology sources. The outputs of these plugins should be validated before giving the user access to choose them. Please see the [deepdive-plugins](http://www.github.com/vsoch/deepdive-plugins) README for details. 

#### Thinking about Workflow
Running this module as an application, it should produce something that can be immediately deployed to do the analysis. However for advanced users, it should be just as easy to use the python module functions, as is, in custom scripts. This seems like a reasonable solution, for now, until there is a way to make a central cloud-based tool that can send computationally intensive jobs to a controller on a cluster, likely with some kind of authenticated REST call. The user should also have the option to use deepdive as it is originall done, running things in serial locally. 

Since I am just developing these functions, to start I think that I will develop the "application instance" approach that configures a pipeline to run on a SLURM cluster after specifying a set of resources. Then I can expand on that.


### First Project
I will start with nifti, meaning using labels from at atlas or brain map, and finding relationships to cognitive concepts (cognitiveatlas) as this seems like the quickest way to connect terms with actual data. I will develop the module to interactively collect my desired inputs, and then deploy the application to the TACC cluster (SLURM launch system).
